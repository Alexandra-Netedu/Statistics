#citeste_txt.py
#with open("corpus.txt", "r", encoding="utf-8") as f:
 #   continut = f.read()

#print("Con»õinutul fi»ôierului este:\n")
#print(continut)

#import re

#with open("corpus.txt", "r", encoding="utf-8") as f:
 #for linie in f:
   #  linie = linie.strip()
        # cautƒÉ: numƒÉr + punct, apoi text √Æntre ghilimele speciale + virgulƒÉ + clasa
#match = re.match(r'^\d+\.\s*[‚Äú"](.*?)[‚Äù"]\s*,\s*(\d+)$', linie)
#if not match:
 #           print(f"Format invalid: {linie}")
#else:
           # text = match.group(1)
          #  clasa = match.group(2)
           # print(f"Text: {text} | Clasa: {clasa}")
#import re
import nltk
#from nltk.tokenize import word_tokenize
#from nltk.classify import NaiveBayesClassifier
#from nltk.classify.util import accuracy

# AsigurƒÉ-te cƒÉ ai descƒÉrcat datele necesare
#nltk.download('punkt')

# 1. Cite»ôte »ôi extrage text + clasa
#texte = []
#clase = []

#with open("corpus.txt", "r", encoding="utf-8") as f:
#    for linie in f:
#        linie = linie.strip()
#        match = re.match(r'^\s*[‚Äú"]?(.*?)[‚Äù"]?\s*,\s*(\d+)$', linie)
#        if match:
#            text = match.group(1)
#            clasa = match.group(2)
#            texte.append(text)
#            clase.append(clasa)

# 2. Transformare √Æn caracteristici (feature extraction)
#def extrage_cuvinte(text):
#    tokens = word_tokenize(text.lower())
#    return {cuvant: True for cuvant in tokens}

# 3. Crearea setului de date
#date_features = [(extrage_cuvinte(text), label) for text, label in zip(texte, clase)]

# 4. √émpƒÉr»õire manualƒÉ √Æn set de antrenare »ôi test
#procent_train = 0.8
#dim_train = int(len(date_features) * procent_train)
#train_set = date_features[:dim_train]
#test_set = date_features[dim_train:]

# 5. Antrenare Naive Bayes
#clasificator = NaiveBayesClassifier.train(train_set)

# 6. Evaluare a acurate»õei
#print("Acurate»õe:", accuracy(clasificator, test_set))



import nltk
#from nltk.corpus import stopwords
#from nltk.tokenize import word_tokenize
#from nltk.stem import WordNetLemmatizer
#import string
#import re

#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')

#stop_words = set(stopwords.words('english'))
#lemmatizer = WordNetLemmatizer()


#def preprocess(text):
     #Lowercase
     #text = text.lower()

     #Tokenize
     #tokens = word_tokenize(text)

     #Remove punctuation and stopwords, lemmatize
     #cleaned_tokens = []
     #for token in tokens:
      #  if token in string.punctuation:
       #     continue
        #if token in stop_words:
         #   continue
        #lemma = lemmatizer.lemmatize(token)
        #cleaned_tokens.append(lemma)
        #return cleaned_tokens


#def extract_features(words):
 #   return {word: True for word in words}


#Read corpus
#data = []
#with open('corpus.txt', 'r', encoding='utf-8') as f:
 #   for line in f:
  #      line = line.strip()
      #   Extract text and label from each line using regex
   #     match = re.match(r'^"(.*)",\s*(\d+)$', line)
    #    if match:
     #       text = match.group(1)
      #      label = match.group(2)
       #     tokens = preprocess(text)
        #    features = extract_features(tokens)
         #   data.append((features, label))

 #Example: Print first 3 preprocessed samples
#for sample in data[:3]:
 #   print(sample)

import nltk
#from nltk.tokenize import word_tokenize
#from nltk.classify import NaiveBayesClassifier
#from nltk.classify.util import accuracy

# exemplu simplu
#texte = [
#    "I feel so happy today!",
#    "I am very sad and depressed.",
 #   "I am angry about what happened.",
  #  "I feel anxious and worried."
#]
#clase = ['fericire', 'tristete', 'furie', 'anxietate']

#def extract_features(text):
#    tokens = word_tokenize(text.lower())
#    return {word: True for word in tokens}

#data = [(extract_features(text), label) for text, label in zip(texte, clase)]

#train_set = data[:int(len(data)*0.8)]
#test_set = data[int(len(data)*0.8):]

#classifier = NaiveBayesClassifier.train(train_set)

#print("Acurate»õe:", accuracy(classifier, test_set))

#classifier.show_most_informative_features(20)

#import re
import nltk
#from nltk.tokenize import word_tokenize
#from nltk.classify import NaiveBayesClassifier
#from nltk.classify.util import accuracy

#nltk.download('punkt')

# 1. Cite»ôte fi»ôierul »ôi extrage text + clasƒÉ
#texte = []
#clase = []

#with open("corpus.txt", "r", encoding="utf-8") as f:
#    for linie in f:
#        linie = linie.strip()
#        match = re.match(r'^\s*[‚Äú"](.*?)[‚Äù"]\s*,\s*(\d+)$', linie)
#        if match:
#            text = match.group(1)
#            clasa = match.group(2)
#            texte.append(text)
#            clase.append(clasa)

# 2. Feature extraction
#def extrage_cuvinte(text):
#    tokens = word_tokenize(text.lower())
#    return {cuvant: True for cuvant in tokens}

# 3. CreeazƒÉ setul de date
#date_features = [(extrage_cuvinte(text), label) for text, label in zip(texte, clase)]

# 4. √émpƒÉr»õire train/test
#train_set = date_features[:int(0.8 * len(date_features))]
#test_set = date_features[int(0.8 * len(date_features)):]

# 5. Antrenare clasificator
#clasificator = NaiveBayesClassifier.train(train_set)

# 6. Acurate»õe
#print("Acurate»õe:", accuracy(clasificator, test_set))

# 7. TrƒÉsƒÉturi informative
#clasificator.show_most_informative_features(20)


#from nltk.corpus import stopwords
#from nltk.stem import WordNetLemmatizer

#stop_words = set(stopwords.words('english'))
#lemmatizer = WordNetLemmatizer()

#def extrage_cuvinte(text):
#    tokens = word_tokenize(text.lower())
#    tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in stop_words]
#    return {cuvant: True for cuvant in tokens}
#import re
import nltk
#from nltk.tokenize import word_tokenize
#from nltk.classify import NaiveBayesClassifier
#from nltk.classify.util import accuracy
from nltk.corpus import stopwords
#nltk.download('punkt')
#nltk.download('stopwords')

# 1. Cite»ôte »ôi extrage text + etichete
#texte = []
#clase = []

#with open("corpus.txt", "r", encoding="utf-8") as f:
 #   for linie in f:
  #      linie = linie.strip()
   #     match = re.match(r'^[‚Äú"](.*?)[‚Äù"]\s*,\s*(\d+)$', linie)
    #    if match:
     #       text = match.group(1)
      #      clasa = match.group(2)
       #     texte.append(text)
        #    clase.append(clasa)

# 2. Extrage caracteristici din text
#def extrage_cuvinte(text):
#    tokens = word_tokenize(text.lower())
#    return {cuvant: True for cuvant in tokens if cuvant.isalpha()}  # ignorƒÉ semnele de punctua»õie

# 3. Set de date cu trƒÉsƒÉturi »ôi etichete
#date_features = [(extrage_cuvinte(text), label) for text, label in zip(texte, clase)]

# 4. √émpƒÉr»õire √Æn set de antrenare »ôi test
#procent_train = 0.8
#limita = int(len(date_features) * procent_train)
#train_set = date_features[:limita]
#test_set = date_features[limita:]

# 5. Antrenare model
#clasificator = NaiveBayesClassifier.train(train_set)

# 6. Evaluare
#print("Acurate»õe:", accuracy(clasificator, test_set))
#clasificator.show_most_informative_features(20)


import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.classify import NaiveBayesClassifier
from nltk.classify.util import accuracy
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# DescƒÉrcare resurse necesare
#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')

# Func»õie de extragere a trƒÉsƒÉturilor
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

from nltk.corpus import stopwords
import string

stop_words = set(stopwords.words('english'))

def extrage_cuvinte(text):
    tokens = word_tokenize(text.lower())
    tokens = [cuvant for cuvant in tokens if cuvant.isalpha() and cuvant not in stop_words]
    return {cuvant: True for cuvant in tokens}


# Cite»ôte corpusul
texte = []
clase = []

with open("corpus.txt", "r", encoding="utf-8") as f:
    for linie in f:
        linie = linie.strip()
        match = re.match(r'^\s*[‚Äú"]?(.*?)[‚Äù"]?\s*,\s*(\d+)\s*$', linie)
        if match:
            text = match.group(1)
            clasa = match.group(2)
            texte.append(text)
            clase.append(clasa)

# PregƒÉtire seturi de antrenare/test
date_features = [(extrage_cuvinte(text), label) for text, label in zip(texte, clase)]
limita = int(len(date_features) * 0.8)
train_set = date_features[:limita]
test_set = date_features[limita:]

# Antrenare model
clasificator = NaiveBayesClassifier.train(train_set)


# Evaluare
print("Acurate»õe:", accuracy(clasificator, test_set))
print("Most Informative Features")
clasificator.show_most_informative_features(20)

# Testare propozi»õie nouƒÉ
#propozitie_noua = input("\nIntrodu o propozi»õie pentru clasificare: ")
#features = extrage_cuvinte(propozitie_noua)
#eticheta = clasificator.classify(features)
#print(f"Eticheta prezisƒÉ: {eticheta} (1=Anxietate, 2=Fericire, 3=Furie, 4=Triste»õe)")

#from sklearn.metrics import confusion_matrix, classification_report

#y_true = [label for (_, label) in test_set]
#y_pred = [clasificator.classify(features) for (features, _) in test_set]

#print(classification_report(y_true, y_pred))


# Set de testare ‚Äì propozi»õii fƒÉrƒÉ etichetƒÉ
#propozitii_test = [
#    "I can‚Äôt stop worrying about what might go wrong.",
#    "Just had the best weekend with friends ‚ù§Ô∏è",
#    "I‚Äôm constantly second-guessing every decision I make.",
#    "I finally achieved my goal and I‚Äôm so proud.",
#    "My heart races every time I think about tomorrow.",
#    "This is the happiest I‚Äôve been in years üòä",
#    "I don‚Äôt know if I can handle this anymore üòü",
#    "It feels like everything is spiraling out of control.",
#    "Stop blaming me for your mistakes! üò§",
#    "I can‚Äôt stop smiling, life feels perfect right now.",
#    "Today has been absolutely amazing!",
#    "I‚Äôm so sick of being treated like I don‚Äôt matter.",
#    "I can‚Äôt believe they did that ‚Äî I‚Äôm furious.",
#    "This is completely unacceptable and I won‚Äôt stand for it.",
#    "I feel like crying and I don‚Äôt know why.",
#    "There‚Äôs this constant sadness I can‚Äôt explain.",
#    "Everything feels pointless lately.",
#]

#print("\nClasificare automatƒÉ pe setul de testare:\n")
#for idx, prop in enumerate(propozitii_test, start=1):
#    features = extrage_cuvinte(prop)
#    eticheta_prezisa = clasificator.classify(features)
#    print(f"{idx}. \"{prop}\" ‚Üí EtichetƒÉ prezisƒÉ: {eticheta_prezisa} (1=Anxietate, 2=Fericire, 3=Furie, 4=Tristete)")

# Set de testare ‚Äì propozi»õii cu etichete reale (1=Anxietate, 2=Fericire, 3=Furie)
propozitii_test_etichetate= [
    ("I can‚Äôt stop worrying about what might go wrong.", "1"),
    ("Just had the best weekend with friends ‚ù§Ô∏è", "2"),
    ("I‚Äôm constantly second-guessing every decision I make.", "1"),
    ("I finally achieved my goal and I‚Äôm so proud.", "2"),
    ("My heart races every time I think about tomorrow.", "1"),
    ("This is the happiest I‚Äôve been in years üòä", "2"),
    ("I don‚Äôt know if I can handle this anymore üòü", "1"),
    ("It feels like everything is spiraling out of control.", "1"),
    ("Stop blaming me for your mistakes! üò§", "3"),
    ("I can‚Äôt stop smiling, life feels perfect right now.", "2"),
    ("Today has been absolutely amazing!", "2"),
    ("I‚Äôm so sick of being treated like I don‚Äôt matter.", "3"),
    ("I can‚Äôt believe they did that ‚Äî I‚Äôm furious.", "3"),
    ("This is completely unacceptable and I won‚Äôt stand for it.", "3"),
    ("The clock kept ticking, but time stopped for me.", "4"),
    ("An empty chair still waits at the table.", "4"),
    ("The echo hurts more than the absence.", "4")
]

print("\nClasificare automatƒÉ pe setul de testare:\n")
corecte = 0
for idx, (prop, eticheta_corecta) in enumerate(propozitii_test_etichetate, start=1):
    features = extrage_cuvinte(prop)
    eticheta_prezisa = clasificator.classify(features)
    corecte += eticheta_prezisa == eticheta_corecta
    print(f'{idx}. "{prop}" ‚Üí EtichetƒÉ prezisƒÉ: {eticheta_prezisa}, EtichetƒÉ realƒÉ: {eticheta_corecta} '
          f'{"‚úÖ" if eticheta_prezisa == eticheta_corecta else "‚ùå"}')

print(f"\nAcurate»õea pe setul de testare manual: {corecte}/{len(propozitii_test_etichetate)} = {corecte / len(propozitii_test_etichetate):.2%}")

with open("rezultate_testare.txt", "w", encoding="utf-8") as f:
    f.write("Clasificare automatƒÉ pe setul de testare:\n\n")
    corecte = 0
    for idx, (prop, eticheta_corecta) in enumerate(propozitii_test_etichetate, start=1):
        features = extrage_cuvinte(prop)
        eticheta_prezisa = clasificator.classify(features)
        corecte += eticheta_prezisa == eticheta_corecta
        rezultat = f'{idx}. "{prop}" ‚Üí EtichetƒÉ prezisƒÉ: {eticheta_prezisa}, EtichetƒÉ realƒÉ: {eticheta_corecta} ' \
                   f'{"‚úÖ" if eticheta_prezisa == eticheta_corecta else "‚ùå"}\n'
        f.write(rezultat)

    f.write(f"\nAcurate»õea pe setul de testare manual: {corecte}/{len(propozitii_test_etichetate)} = "
            f"{corecte / len(propozitii_test_etichetate):.2%}\n")
